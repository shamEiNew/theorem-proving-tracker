<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Neuro-Symbolic AI + Lean Roadmap (Mathematician's Track)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    :root {
      --bg: #ffffff;
      --fg: #000000;
      --border: #d0d0d0;
      --header-bg: #000000;
      --header-fg: #ffffff;
      --accent: #111111;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      padding: 0;
      background: var(--bg);
      color: var(--fg);
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      line-height: 1.5;
    }

    .wrapper {
      max-width: 1200px;
      margin: 0 auto;
      padding: 24px 16px 48px;
    }

    h1 {
      font-size: 1.8rem;
      margin: 0 0 0.25rem;
      font-weight: 600;
    }

    h2 {
      font-size: 1.1rem;
      font-weight: 500;
      margin: 1.5rem 0 0.5rem;
      color: var(--accent);
    }

    .subtitle {
      font-size: 0.95rem;
      color: #555;
      margin-bottom: 1rem;
    }

    .meta {
      font-size: 0.8rem;
      color: #777;
      margin-bottom: 1.25rem;
    }

    .legend {
      font-size: 0.8rem;
      color: #555;
      margin-bottom: 0.75rem;
    }

    .legend span {
      display: inline-block;
      margin-right: 12px;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      table-layout: fixed; /* ensures wrapping */
      border: 1px solid var(--border);
      font-size: 0.85rem;
    }

    thead {
      background: var(--header-bg);
      color: var(--header-fg);
    }

    th, td {
      border: 1px solid var(--border);
      padding: 8px 6px;
      vertical-align: top;
      word-wrap: break-word;
      white-space: normal;
    }

    th {
      text-align: left;
      font-weight: 500;
    }

    tr:nth-child(even) td {
      background: #fafafa;
    }

    tr:nth-child(odd) td {
      background: #ffffff;
    }

    .col-week {
      width: 50px;
    }

    .col-stage {
      width: 120px;
    }

    .col-hours {
      width: 60px;
      text-align: center;
    }

    .col-completed {
      width: 70px;
      text-align: center;
    }

    .col-remarks {
      width: 140px;
    }

    input[type="checkbox"] {
      width: 16px;
      height: 16px;
      cursor: pointer;
    }

    .small-note {
      font-size: 0.8rem;
      color: #777;
      margin-top: 0.5rem;
    }

    @media (max-width: 768px) {
      table {
        font-size: 0.8rem;
      }
      h1 {
        font-size: 1.4rem;
      }
    }
  </style>
</head>
<body>
  <div class="wrapper">
    <h1>Neuro-Symbolic AI + Lean Roadmap (Mathematician's Track)</h1>
    <div class="subtitle">
      16-week accelerated plan: Leveraging mathematical maturity and Python skills to build AI tools for Theorem Proving.
    </div>
    <div class="meta">
      Phases: P1 Lean Foundations · P2 The Python-Lean Bridge · P3 Deep Learning & RAG · P4 Research & Portfolio.
    </div>

    <div class="legend">
      <span><strong>Hours/week:</strong> suggested 10–12 hours</span>
      <span><strong>Goal:</strong> Build an AI-assisted proving tool</span>
      <span><strong>Focus:</strong> Automation & Benchmarks (MiniF2F)</span>
    </div>

    <h2>Week-by-Week Plan (with Deliverables)</h2>

    <table>
      <thead>
        <tr>
          <th class="col-week">Week</th>
          <th class="col-stage">Phase</th>
          <th>Primary Focus</th>
          <th>Goals for the Week</th>
          <th>Deliverable (GitHub / Notebooks)</th>
          <th>Core References / Resources</th>
          <th class="col-hours">Hours</th>
          <th class="col-completed">Done</th>
          <th class="col-remarks">Remarks</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>1</td>
          <td>P1 — Foundation</td>
          <td>Quantifiers as Functions</td>
          <td>
            Finish TPiL4 Chapter 4 (Quantifiers). <br>
            Internalize the Curry-Howard correspondence (Proofs = Programs). <br>
            <strong>Key concept:</strong> `∀` is a Pi-type (dependent function).
          </td>
          <td>
            <em>week1-quantifiers.lean</em>: Manually implement `and_comm` and `quantifier_swap` without tactics.
          </td>
          <td>
            - TPiL4 (Ch 4)
            - "Propositions as Types" (Wadler)
          </td>
          <td class="col-hours">8–10</td>
          <td class="col-completed"><input type="checkbox" checked></td>
          <td>Critical for AI intuition.</td>
        </tr>

        <tr>
          <td>2</td>
          <td>P1 — Foundation</td>
          <td>Tactics & Induction</td>
          <td>
            Master `induction`, `rewrite`, and `simp`. <br>
            Complete the Natural Number Game (NNG) to build "tactic intuition" (needed for generating data later).
          </td>
          <td>
            Screenshot of NNG Certificate + <em>week2-tactics.lean</em> with commented tactic states.
          </td>
          <td>
            - TPiL4 (Ch 5-8)
            - Natural Number Game (Lean 4)
          </td>
          <td class="col-hours">8–10</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Gamified learning.</td>
        </tr>

        <tr>
          <td>3</td>
          <td>P1 — Foundation</td>
          <td>Benchmarks (MiniF2F/IMO)</td>
          <td>
            <strong>Do not formalize a book.</strong> <br>
            Read <em>Mathematics in Lean</em> to learn API basics, then formalize 3 distinct problems from the <strong>MiniF2F</strong> benchmark (Algebra, Number Theory, Topology).
          </td>
          <td>
            <em>benchmarks.lean</em>: Formal statements and proofs for 3 MiniF2F/IMO-style problems.
          </td>
          <td>
            - MiniF2F Repository (GitHub)
            - Mathematics in Lean
          </td>
          <td class="col-hours">8–10</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Aligns with AI eval standards.</td>
        </tr>

        <tr>
          <td>4</td>
          <td>P1 — Foundation</td>
          <td>Lean 4 Metaprogramming</td>
          <td>
            <strong>The Pivot:</strong> Stop proving; start hacking. <br>
            Learn `Lean.Expr` (Expression tree). <br>
            Write a tactic that inspects the goal state.
          </td>
          <td>
            <em>week4-meta.lean</em>: A tactic `elab "check_type"` that prints the current goal's type to the console.
          </td>
          <td>
            - Lean 4 Metaprogramming Book
            - "Macro" vs "Elab" chapters
          </td>
          <td class="col-hours">10–12</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Hardest week. Don't skip.</td>
        </tr>

        <tr>
          <td>5</td>
          <td>P2 — The Bridge</td>
          <td>"Hello World" Python-Lean</td>
          <td>
            Install `lean-interact` or `repl`. <br>
            Write a Python script that starts a Lean server and sends a command string.
          </td>
          <td>
            <em>bridge.py</em>: Script that sends "1+1" to Lean and parses the output "2".
          </td>
          <td>
            - lean-interact (PyPI)
            - Lean REPL documentation
          </td>
          <td class="col-hours">6–8</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Proof of concept.</td>
        </tr>

        <tr>
          <td>6</td>
          <td>P2 — The Bridge</td>
          <td>LeanDojo (Industrial Tooling)</td>
          <td>
            Install <strong>LeanDojo</strong>. <br>
            Use it to trace a repo and extract (Theorem, Proof) pairs. <br>
            Understand the `traced_repo` object.
          </td>
          <td>
            <em>extract_data.py</em>: Generates a JSON file with 100 theorems and their tactic steps from Mathlib.
          </td>
          <td>
            - LeanDojo Paper
            - LeanDojo GitHub / Docs
          </td>
          <td class="col-hours">8–10</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Standard research tool.</td>
        </tr>

        <tr>
          <td>7</td>
          <td>P2 — The Bridge</td>
          <td>Premise Selection (Data Eng)</td>
          <td>
            <strong>Problem:</strong> Which lemma to use? <br>
            Build a "Semantic Search" tool. <br>
            Use TF-IDF or OpenAI Embeddings on theorem statements.
          </td>
          <td>
            <em>retrieve_lemma.py</em>: Input a theorem string, output top-5 relevant Mathlib lemma names.
          </td>
          <td>
            - "HyperTree Proof Search" (Paper)
            - scikit-learn (TF-IDF)
          </td>
          <td class="col-hours">8–10</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Critical for RAG.</td>
        </tr>

        <tr>
          <td>8</td>
          <td>P2 — The Bridge</td>
          <td>The Gym (Eval Loop)</td>
          <td>
            Create the "Agent Loop": <br>
            1. Python reads state. <br>
            2. Python proposes tactic (random for now). <br>
            3. Lean executes. <br>
            4. Python gets feedback.
          </td>
          <td>
            <em>agent_loop.py</em>: A script that tries `simp`, `ring`, `linarith` on a goal until one works.
          </td>
          <td>
            - OpenAI Gym interface concepts
            - LeanDojo `Dojo` class
          </td>
          <td class="col-hours">8–10</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Foundation for RL.</td>
        </tr>

        <tr>
          <td>9</td>
          <td>P3 — Neuro Layer</td>
          <td>Deep Learning Fast Track</td>
          <td>
            Learn Seq2Seq architectures. <br>
            Understand "Attention" mechanisms (Transformers). <br>
            Map "English->French" to "Theorem->Proof".
          </td>
          <td>
            <em>seq2seq_tutorial.ipynb</em>: Complete the PyTorch Translation tutorial.
          </td>
          <td>
            - PyTorch Seq2Seq Tutorial
            - "The Illustrated Transformer"
          </td>
          <td class="col-hours">10–12</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Theory week.</td>
        </tr>

        <tr>
          <td>10</td>
          <td>P3 — Neuro Layer</td>
          <td>LLM Fine-Tuning</td>
          <td>
            Use HuggingFace `transformers`. <br>
            Fine-tune a small model (T5 or ByT5) on your Week 6 dataset. <br>
            Task: Input State -> Output Tactic.
          </td>
          <td>
            <em>train_prover.py</em> + Training Loss Curve.
          </td>
          <td>
            - HuggingFace Trainer API
            - LeanDojo Benchmark datasets
          </td>
          <td class="col-hours">10–12</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>First "AI" model.</td>
        </tr>

        <tr>
          <td>11</td>
          <td>P3 — Neuro Layer</td>
          <td>Retrieval Augmented Generation (RAG)</td>
          <td>
            Connect Week 7 (Retriever) with Week 10 (Generator). <br>
            Prompt: "Given these lemmas [L1, L2], prove [Goal]".
          </td>
          <td>
            <em>rag_prover.py</em>: A script that retrieves premises before generating a tactic.
          </td>
          <td>
            - "LeanDojo: RAG for Theorems"
            - FAISS (Facebook AI Similarity Search)
          </td>
          <td class="col-hours">8–10</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>State of the art.</td>
        </tr>

        <tr>
          <td>12</td>
          <td>P3 — Neuro Layer</td>
          <td>Tree Search (Inference)</td>
          <td>
            LLMs make mistakes. Implement Search. <br>
            Use Best-First Search (BFS) where the LLM "scores" the branches.
          </td>
          <td>
            <em>proof_search.py</em>: Implements a search tree allowing backtracking when tactics fail.
          </td>
          <td>
            - "AlphaProof" concepts
            - Graph Theory (Trees)
          </td>
          <td class="col-hours">10–12</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>The "Brain" of the agent.</td>
        </tr>

        <tr>
          <td>13</td>
          <td>P4 — Portfolio</td>
          <td>Community & Landscape</td>
          <td>
            Read recent AITP papers (2024-2025). <br>
            Join Lean Zulip `#machine-learning`. <br>
            Identify a gap (e.g., "AI is bad at inequality proofs" or "AI struggles with MiniF2F algebra").
          </td>
          <td>
            <em>lit_review.md</em>: Summary of 3 top papers and where your tool fits.
          </td>
          <td>
            - AITP Conference Proceedings
            - arXiv (Neuro-symbolic AI)
          </td>
          <td class="col-hours">6–8</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Contextualize your work.</td>
        </tr>

        <tr>
          <td>14</td>
          <td>P4 — Portfolio</td>
          <td>Capstone: "Auto-Formalizer" or "Copilot"</td>
          <td>
            Build a user-facing tool. <br>
            Example: A CLI tool where you type a math problem, and it suggests the Lean code.
          </td>
          <td>
            <em>MVP v0.1</em>: A working script demonstrating the end-to-end pipeline on unseen problems.
          </td>
          <td>
            - Your codebase (Weeks 5-12)
            - VS Code Extension API (optional)
          </td>
          <td class="col-hours">12–15</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>The "Money" project.</td>
        </tr>

        <tr>
          <td>15</td>
          <td>P4 — Portfolio</td>
          <td>Polish & Documentation</td>
          <td>
            Clean up code. Add docstrings. <br>
            Write a technical blog post: "How I taught an AI to prove Math using LeanDojo."
          </td>
          <td>
            Public GitHub Repo with nice README + Medium/Substack post.
          </td>
          <td>
            - "Write the Docs" guide
            - Distill.pub style
          </td>
          <td class="col-hours">8–10</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Visibility > Code quality.</td>
        </tr>

        <tr>
          <td>16</td>
          <td>P4 — Portfolio</td>
          <td>Outreach & Application</td>
          <td>
            Update CV: "Neuro-Symbolic AI Engineer". <br>
            Email labs/startups with your Blog Post + Repo.
          </td>
          <td>
            <em>applications.md</em>: List of 5 labs/companies (e.g., Symbolica, DeepMind) to contact.
          </td>
          <td>
            - "Cold Emailing for Research"
            - LinkedIn
          </td>
          <td class="col-hours">5–7</td>
          <td class="col-completed"><input type="checkbox"></td>
          <td>Launch.</td>
        </tr>
      </tbody>
    </table>

    <div class="small-note">
      <strong>Crucial Note:</strong> Phase 1, Week 3 is intentionally set to "Benchmarks" (MiniF2F/IMO) rather than "Textbook Formalization." 
      Formalizing an entire book (like Munkres or Dummit & Foote) is a massive time sink with low ROI for AI Engineering. 
      Formalizing benchmark problems directly prepares you for the way AI models are evaluated in industry.
    </div>
  </div>
</body>
</html>